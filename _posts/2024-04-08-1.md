---
title: LLM(Large Language Models)의 문제점과 해결책1, RLHF
date: 2024-04-08
categories: [DL, LLM]
tags: [GPT, RLHF]     # TAG names should always be lowercase
math: true
mermaid: true # 그래프 도구
image:
  path: 'https://github.com/5iye/covidPredict/assets/112414191/1064a6a4-90ef-4106-80ed-cd52c0807053'
#   alt: image alternative text
---
## Language Model과 상호작용 방식

1. 텍스트 기반으로서 모델에게 텍스트와 명령문을 제공한다.  
     - 주어진 텍스트를 한국어로 번역해줘
2. **모델의 내부지식을 기반으로 답변 생성한다.**
     - 독감의 원인은?
3. 창의적인 결과를 생성한다.
    - 아보카도와 컴퓨터로 시를 작성해줘

## <span style="background-color:#fff5b1">  LLM의 문제점: 모델의 내부 지식에 무엇이 포함되어 있는지 알 수 없다. </span>

- **RLHF** (Reinforcement Learning From Human Feedback)
    - 지도학습(Supervised Learning)으로 language model을 학습시킬 경우, 모델은 어떻게든 내부 지식을 기반으로 답변과 질문을 연관시켜 ‘거짓말’을 하도록 장려한다.
    - <span style="color:red"> RL을 통해 모르는 질문에 대해서 “모르겠어요”라고 답을 할 수 있도록 학습한다. </span>
    - “답변 포기” 점수보다 “오답” 점수에 더 처벌을 가하여 모델이 절제하는 방법을 배우도록 한다.
    

- **RAG** (Retrieval-Augmented Generation)
    - 기본 모델 자체를 수정하지 않고, retriever를 사용하여 최선 정보나 특정 산업에 국한된 정보를 증거를 토대로 더 나은 답변을 제공한다. (업로드 예정)



    
## **InstructGPT의 RLHF**

![img-description](https://github.com/5iye/covidPredict/assets/112414191/1064a6a4-90ef-4106-80ed-cd52c0807053)


1. Supervised Fine-Tuning(SFT)
    - demo 데이터셋 (선택된 프롬프트-인간 Labeler의 답변, 12~15K)으로 fine-tuning
        - 비용이 많이 들고 느림
        - 작지만 고품질 선별
        - alignment 오류 발생
2. Reward Model(Mimic Human Preferences)  ← 강화학습이 아님
    - 인간 Labeler가 1번 모델의 출력에 점수를 매긴 데이터셋(30~40K)으로 Reward Model 학습
        - <span style="color:yellowgreen"> 인간의 선호도를 모방한 형태 </span>
        - 인간 Labeler가 여러 출력을 만드는 것보다 쉽고 효율적
3. PBO(Proximal Policy Optimization)를 이용한 SFT 모델 강화학습
    

    ![img-description](https://github.com/5iye/covidPredict/assets/112414191/91081e9c-fcc9-4691-a4ae-046d96225537)
    
    - PBO(Proximal Policy Optimization)
        - **ratio**를 도입하여 step 단위로 importance를 계산함
        - <span style="color:yellowgreen"> new probs가 클수록, initial probs가 작을수록 </span> importance가 커짐
    - *R = reward + KL*
        - trainable SFT 모델과 frozen SFT 모델 사이에 KL divergence를 계산함
            - trainable SFT가 말도 안되는 텍스트를 출력하는 것을 방지하기 위해
    - reward loss : <span style="color:red"> **ratio** 와 **R** 이 모두 커지도록 </span>
        - 기존 2단계 reward model은 입력을 로그확률을 디코딩한 text로 받는데, 이 디코딩 프로세스는 미분할 수 없었음
        - PBO 동작을 통해 loss가 미분가능해짐 → SFT Model의 weight 업데이트 가능



