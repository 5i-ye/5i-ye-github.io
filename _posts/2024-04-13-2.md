---
title: Clustering(군집화)의 모든 것
date: 2024-04-13
categories: [ML]
tags: [Clustering]     # TAG names should always be lowercase
math: true
mermaid: true # 그래프 도구
image:
  path: 'https://github.com/5iye/covidPredict/assets/164887360/b9a83ad5-9703-4c7b-a1f9-23526dc03041'
#   alt: image alternative text
---
### **군집화 할 때 결정해야할 것**

1. 데이터 간의 <span style="color:red">거리</span>를 어떻게 측정할 거냐
2. 어떤 군집화 <span style="color:red">방법</span>을 사용할 거냐
3. 군집의 <span style="color:red">개수</span>를 어떻게 결정할 거냐

## 1.  거리 측정 방법

### **유클리드 거리 이용**

$$
d(X, Y) = \sqrt{\sum\limits_{i=1}^n (X_i - Y_i)^2}
$$

### **상관계수 거리 이용**

$$
d_{corr}(X, Y) = 1-𝜎_{xy}, -1 \leq 𝜎_{xy} \leq 1
$$

![상관계수](https://github.com/5iye/covidPredict/assets/164887360/7eb092a7-13ac-45e2-84bb-9030ff7d7f85){: width="400" height="150"}
```python
array # (n_군집, n_feature)
corr_matrix = pd.DataFrame(array).T.corr()
dis = 1-corr_matrix
```

### **코사인 거리 이용**

$$
d_{cos}(X,Y) = 1-cos\theta
$$

$$
\cos\theta = \frac{A \cdot B}{\left\|\ A \right\| \left\|\ B \right\|} = \frac {\sum_{i=1}^n A_i \times B_i}{\sqrt{\sum_{i=1}^n (A_i)^2} \times \sqrt{\sum_{i=1}^n (B_i)^2}}
$$

![코사인 유사도](https://github.com/5iye/covidPredict/assets/164887360/3338f9d3-7b4a-4e7e-9f4b-56491b618e64)

```python
from sklearn.metrics.pairwise import cosine_similarity

cos_matrix = cosine_similarity(arr)
dis = 1-cos_matrix
```

## 2. 군집화 방법

### **K-means**

- 방법론
    
    ![kmeans](https://github.com/5iye/covidPredict/assets/164887360/f1f163dc-4719-4d04-8182-2484d74947fb)
    
    1. k개의 임의의 중심값을 고른다. 
    2. 중심값까지 거리 계산을 통해 cluster 할당한다.
    3. cluster에 속한 데이터들의 평균값으로 중심값 이동한다.
    4. 데이터에 대한 cluster 할당이 변하지 않을 때까지 반복한다.

- 코드
    
    ```python
    from sklearn.cluster import KMeans
    
    # n_clusters == k
    kmeans = KMeans(n_clusters, init='random')   # init: 디폴트 k-means++ (그럴듯한 위치에)
    kmeans.fit(x)
    
    kmeans.labels_               # 학습이 끝난 후 할당된 cluster
    kmeans.cluster_centers_      # 학습이 끝났을 때 중심값 확인
    
    kmeans.predict(x_test)       # 새로운 데이터 예측
    ```
    

### **Hierarchical_clustering (계층적 군집 분석)**

**中 AgglomerativeClustering (상향식, 응집형)**

- 방법론
    1. 각 노드를 하나의 클러스터라고 생각하고 거리가 가까운 것끼리 묶는다.
    2. 1번을 반복한다.
    
    이때, 묶는 방법 중 하나인 ‘ward’ 란, <span style="background-color:#fff5b1">두 군집이 합쳐졌을 때 오차 제곱합의 증가분</span>을 거리로 측정한다.
    
    - 노이즈나 이상치에 덜 민감함, 비슷한 크기의 군집끼리 묶어줌
        ![ward](https://github.com/5iye/covidPredict/assets/164887360/b9a83ad5-9703-4c7b-a1f9-23526dc03041){: width="400" height="250"}{: left }
        
- 코드
    
    ```python
    from sklearn.cluster import AgglomerativeClustering
    
    cluster = AgglomerativeClustering(n_clusters=k, linkage='ward')
    labels = cluster.fit_predict(matrix)
    ```
    

- 시각화 - dendrogram
    
    ```python
    from scipy.cluster.hierarchy import dendrogram, linkage
    
    Z = linkage(matrix, 'ward')
    plt.figure(figsize=(10, 5))
    dendrogram(Z)
    plt.show()
    ```
    
    ![dendrogram](https://github.com/5iye/covidPredict/assets/164887360/a1fcd71c-fc7f-4883-bb7c-fdd196a0bb83)
    

## 3. 최적 클러스터 개수(k) 찾기

### 엘보우 기법

각 데이터마다 자신이 속한 cluster 의 중심값까지의 거리의 제곱의 합(inertia)을 계산 

→ 잘 적용된 cluster라면 거리가 짧음, 서로 다른 cluster끼리 얼마나 떨어져 있는지에 대한 정보가 없음

![elbow](https://github.com/5iye/covidPredict/assets/164887360/4eb285d0-c82d-4a31-8477-839b7789497d)

```python
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer

k=0
kmeans = KMeans(n_clusters=k, random_state=7)
visualizer = KElbowVisualizer(kmeans, k=(1,10), timings=False)
visualizer.fit(arr1)
visualizer.show()
```

### **실루엣(silhouette)** 기법

$$
s^i = \frac{b^i - a^i}{max\{a^i, b^i\}}
$$

a: 클러스터 내 데이터 **응집도(cohesion) **

= 데이터 x(i)와 <span style="color:yellowgreen">동일한 클러스터 </span>내의 나머지 데이터들과의 평균 거리 (이상적: 0)

b: 클러스터 간 **분리도(separation) **

= 데이터 x(i)와 <span style="color:yellowgreen">가장 가까운 클러스터 </span>내의 모든 데이터들과의 평균 거리 (커질수록 좋음)

→ <span style="background-color:#fff5b1">1(가장 이상적인 값)에 가까울수록 </span>군집화가 잘되었다고 판단(0.7이상)

```python
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.cluster import AgglomerativeClustering

for k in range(2, 10):
    agg = AgglomerativeClustering(n_clusters=k).fit(matrix)
    silhouette_avg = silhouette_score(matrix, agg.labels_)
    print(f'cluster : {k} // silhouette index {silhouette_avg}')

```