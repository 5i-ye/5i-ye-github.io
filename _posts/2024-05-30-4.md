---
title: Batch Normalization 효과, 단점 및 Layer Norm, Instance Norm, Group Norm의 특징
date: 2024-05-30
categories: [DL]
tags: [Batch Normalization]     # TAG names should always be lowercase
math: true
mermaid: true # 그래프 도구
image:
  path: 'https://github.com/5i-ye/gitblog_img/assets/164887360/8c082901-10c6-4749-814b-9bbec940d85b'
#   alt: image alternative text
---

## Batch Normalization이란?

우리는 data의 크기로 인해 학습이 치우쳐져 있는 것을 방지하고자 input data를 정규화한다는 사실을 알고 있다. 하지만, input 데이터가 특정 분포를 갖더라도 weight가 너무 크거나 작아지면 선형결합의 결과는 치우칠 수 밖에 없다. 이는 학습의 속도를 느리게하고 불안정하게 하는 원인이다.

이를 방지하고자, input data 뿐만 아니라 <span style="background-color:#fff5b1">“모든 hidden layer”</span>를 정규화 하는 과정을 Batch Normalization 이라 한다.
= <span style="background-color:#fff5b1">covariate shift</span>

Batch Norm은 선형 결합 후 activation fn에 들어가기 전에 사용하는 것이 일반적이나 논란 중에 있다.

- 과정
    1. $\mu = \frac{1}{m}\sum_{i=1}^m{x^i}, \sigma^2 = \frac{1}{m}\sum_{i=1}^m{(x^i - \mu)^2}$   계산
    2. $\hat{x_i} = \frac{x_i-\mu}{\sqrt{\sigma^2 + \epsilon}},$  $\epsilon$ ← 분모가 0이 되지 않기 위한 아주 작은 값
    3. $\hat{y_i} = \gamma * \hat{x_i} + \beta$ 
        1. $\gamma, \beta$ ← trainable parameter 로서 각각 scaling과 shift의 역할을 한다. saturation을 통해 유연성을 얻을 수 있다.
        2. sigmoid 예시
            
            ![scaling&shift](https://github.com/5i-ye/gitblog_img/assets/164887360/870c66f1-b1de-464b-b34c-ea9edab0e103)
            {: width="100" height="70"}
            
        3. $\gamma = \sigma^2, \beta = \mu$ 를 넣으면 원상복구
   
   왜 기껏 정규화하고 scaling & shifting 과정을 거치는지 이해가 안갈 수 있다. activation fn으로 sigmoid를 썼다고 해보자. 그렇다면 sigmoid(-1) = 0.2, sigmoid(1) = 0.7 로서, <span style="color:red">비선형성을 띠지 못함</span>을 확인 가능하다.

- 단점
    
    ![layer norm & batch norm in NLP](https://github.com/5i-ye/gitblog_img/assets/164887360/8c082901-10c6-4749-814b-9bbec940d85b)
    _LayerNorm & BatchNorm in NLP_
    
    1. <span style="background-color:#fff5b1">batch size가 작으면 성능 저하된다.</span>
    2. <span style="background-color:#fff5b1">Sample-Based SGD와 RNN에서 사용 불가하다.</span>
        - seq_len에 따라 layer의 추정치가 달라지기 때문

## Layer Normalization, Instance Normalization, Group Normalization 등장

![bn, ln, in, gn in CV](https://github.com/5i-ye/gitblog_img/assets/164887360/215751ba-11a1-423a-8975-70e60c4f1616)
_Normalization in CV_

**Group Norm** 은 **Layer Norm** (G=C)와 **Instance Norm**(G=1)의 중간 방법론이다.
이 3가지 모두 앞선 Batch Normm의 단점 ①, ② 을 해결해준다. 뿐만 아니라, batch가 계산될 때까지 mean, var 저장 공간이 필요 없어 메모리 측면에서도 유리하다.

**Layer Norm**은 나머지와 다른 구별되는 특징이 있다. **Batch Norm, Instance Norm**은 batch(=파란 부분) 별로 mean과 var이 존재한다. 하지만, **Layer Norm**은 <span style="color:red">각 픽셀 별 파라미터를 모든 batch에 공유한다.</span>
 - parameter 수: 2*C*H*W
  ![layernorm layer](https://github.com/5i-ye/gitblog_img/assets/164887360/b1fbc32e-a456-4a10-8842-cda227ee62d8)
 
   - eps=1e-12 ← $\epsilon$, elementwise_affine=True ← $\gamma, \beta$ 파라미터 학습 여부
    
참고 링크 

[https://wandb.ai/wandb_fc/Normalization/reports/Normalization-Series-What-is-Batch-Norm---VmlldzoxMjk2ODcz](https://wandb.ai/wandb_fc/Normalization/reports/Normalization-Series-What-is-Batch-Norm---VmlldzoxMjk2ODcz)

[https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1](https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1)