---
title: BERT CLS vector 추출 및 vector space 시각화
date: 2024-06-20
categories: [DL, LLM]
tags: [BERT]     # TAG names should always be lowercase
math: true
mermaid: true # 그래프 도구
image:
  path: 'https://github.com/5i-ye/gitblog_img/assets/164887360/03047483-231d-4bd8-a050-470669dc772d'
#   alt: image alternative text
---

## BERT란?

**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** 논문에서 등장했으며 텍스트 데이터로 사전 훈련된 언어 모델입니다.

### 방법

1. **Pre-training (unlabeled data)**
    1. MLM
        
        ![MLM](https://github.com/5i-ye/gitblog_img/assets/164887360/25bf3d6e-04ed-455f-904a-b54175097940)
        
    2. NSP
        
        ![NSP](https://github.com/5i-ye/gitblog_img/assets/164887360/56ad9804-0abb-4be6-9e37-2583e6ce89ff)
        
2. **Fine-Tuning (with labeled data)**
    
    ![BERT](https://github.com/5i-ye/gitblog_img/assets/164887360/03047483-231d-4bd8-a050-470669dc772d)
    
    - 레이블이 있는 다른 작업(Task)에서 추가훈련
    - 다양한 downstream task로 확장이 용이

## CLS 토큰이란?

BERT는 방대한 데이터로 학습하였기 때문에 특히 레이블이 있는 <span style="background-color:#fff5b1">텍스트 분류 문제</span>에서 높은 성능을 보입니다. 

![KcElectra model](https://github.com/5i-ye/gitblog_img/assets/164887360/f89afa30-e379-48a1-a940-263102da188b)

사전 훈련된 BERT output의 크기는 (batch_size, max_token, 768) 입니다. 이때, 0번째 토큰 즉 <span style="background-color:#fff5b1">(batch_size, 0, 768) 을 CLS 토큰</span>이라고 부릅니다. 텍스트 분류 문제는 CLS 토큰에서 num_label*768 행렬의 parameter를 학습시키기 때문에 데이터가 분류될 때 매우 중요하며, 분석할 때도 용이합니다.

아래는 CLS vector 추출 코드입니다.

## BERT의 CLS vector 추출 코드 및 시각화

```python
    result = np.empty((0,768))
    
    extracted_features = []
    def hook(module, input, output):
        extracted_features.append(output.to(device)) #output: (b, 512, 768)

    # ------ change CLS token position
    hook_handle = model.module.electra.encoder.layer[-1].output.LayerNorm.register_forward_hook(hook)

    with torch.no_grad():
        for step, batch in enumerate(dataloader):
	        # hook을 걸어놨기 때문에 아래 과정에서 extracted_features에 vector가 저장 
	        outputs = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))
	        CLS_vector = torch.cat([features.to(device)[:, 0, :] for features in extracted_features], dim=0)
	        result = np.concatenate([result, CLS_vector.cpu().detach().numpy()])
		    hook_handle.remove()
```

이렇게 하면 텍스트마다 CLS 토큰 (768차원 벡터) 이 result에 저장된 것을 확인 할 수 있습니다.

아래는 CLS vector를 벡터공간(vector space)에 나타낸 그림입니다. 

저차원으로 줄이는데 PacMap 방식을 채택했습니다. [https://github.com/YingfanWang/PaCMAP](https://github.com/YingfanWang/PaCMAP)

![vector space](https://github.com/5i-ye/gitblog_img/assets/164887360/041b7f08-eec1-42c1-a5ee-ffcb651e5365)

정확도 81% 달성한 이진분류 모델인데 꽤 차이가 두드러지는 부분과 혼돈되어있는 부분을 파악할 수 있었습니다.